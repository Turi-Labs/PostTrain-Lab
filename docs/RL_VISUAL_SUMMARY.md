# RL Training: Visual Summary Guide

> **Quick visual reference** for understanding how different RL methods work

---

## ğŸ“Š The Big Picture: All RL Recipes Compared

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RL TRAINING METHODS                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

METHOD          TASK              TURNS    COMPLEXITY    REWARD SOURCE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RL Basic        Math Problems      1       â˜…â˜†â˜†â˜†â˜†        Symbolic checker
(GSM8K)                                                  (automatic)

Math RL         Math Problems      1       â˜…â˜…â˜†â˜†â˜†        Symbolic checker
(MATH/          (harder)                                 (automatic)
Polaris)

Code RL         Write Code         1       â˜…â˜…â˜…â˜†â˜†        Test execution
(LiveCodeBench)                                          (automatic)

RLVR           Code w/ Tests       1       â˜…â˜…â˜…â˜†â˜†        Test execution
(MBPP)                                                   (automatic)

Guess Number    Number Game       2-10     â˜…â˜…â˜…â˜…â˜†        Game rules
                                                         (programmatic)

Twenty          Q&A Game          2-20     â˜…â˜…â˜…â˜…â˜†        Other AI
Questions                                                (model-based)

Tic-Tac-Toe     Board Game        3-9      â˜…â˜…â˜…â˜…â˜…        Game outcome
(Self-Play)                                              (adversarial)
```

---

## ğŸ”„ The Universal RL Loop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EVERY RL METHOD FOLLOWS THIS                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  1. PRESENT PROBLEM                         â”‚
     â”‚  "Here's a question/task"                   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  2. GENERATE MULTIPLE ATTEMPTS               â”‚
     â”‚  Model tries 4-16 different solutions       â”‚
     â”‚                                             â”‚
     â”‚  Attempt 1: Solution A                      â”‚
     â”‚  Attempt 2: Solution B                      â”‚
     â”‚  Attempt 3: Solution C                      â”‚
     â”‚  Attempt 4: Solution D                      â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  3. EVALUATE EACH ATTEMPT                   â”‚
     â”‚  Check if solutions work                    â”‚
     â”‚                                             â”‚
     â”‚  Solution A: âœ… Reward: +1.0                â”‚
     â”‚  Solution B: âŒ Reward: 0.0                 â”‚
     â”‚  Solution C: âœ… Reward: +1.0                â”‚
     â”‚  Solution D: âŒ Reward: -0.5                â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  4. COMPUTE ADVANTAGES                      â”‚
     â”‚  Compare to average performance             â”‚
     â”‚                                             â”‚
     â”‚  Mean reward: 0.375                         â”‚
     â”‚  Solution A: +0.625 (reinforce!)            â”‚
     â”‚  Solution B: -0.375 (discourage)            â”‚
     â”‚  Solution C: +0.625 (reinforce!)            â”‚
     â”‚  Solution D: -0.875 (really discourage!)    â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  5. UPDATE MODEL                            â”‚
     â”‚  Adjust to favor good solutions             â”‚
     â”‚                                             â”‚
     â”‚  â¬† Increase prob of A and C                 â”‚
     â”‚  â¬‡ Decrease prob of B and D                 â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  6. REPEAT                                  â”‚
     â”‚  Model is now slightly better!              â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Single-Turn vs Multi-Turn

### Single-Turn (Simpler)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Question â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model   â”‚ Generates ONE response
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Answer  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Reward  â”‚ Score the answer
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Examples: Math problems, Code generation
```

### Multi-Turn (Complex)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Question â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model   â”‚ Response 1
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feedback â”‚ "Higher!"
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model   â”‚ Response 2
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feedback â”‚ "Lower!"
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â†“
    ...
     â”‚
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Reward  â”‚ Based on full episode
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Examples: Guess the Number, Twenty Questions
```

---

## ğŸ“ˆ Learning Curves: What to Expect

### RL Basic (Math)

```
Reward
  ^
1.0 |                    â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    |                â•­â”€â”€â”€â•¯
0.5 |           â•­â”€â”€â”€â”€â•¯
    |      â•­â”€â”€â”€â”€â•¯
0.0 |â”€â”€â”€â”€â”€â”€â•¯
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Iteration
        1    5    10   15   20
        
What's happening:
â€¢ Iterations 1-5: Learning format (boxed answers)
â€¢ Iterations 6-10: Learning basic math
â€¢ Iterations 11-15: Developing strategies  
â€¢ Iterations 16-20: Fine-tuning
```

### Code RL

```
Pass Rate (%)
  ^
60% |                            â•­â”€â”€
    |                      â•­â”€â”€â”€â”€â”€â•¯
40% |                â•­â”€â”€â”€â”€â”€â•¯
    |          â•­â”€â”€â”€â”€â”€â•¯
20% |    â•­â”€â”€â”€â”€â”€â•¯
    |â”€â”€â”€â”€â•¯
 0% â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Iteration
        10   30   50   70   100
        
What's happening:
â€¢ Iterations 1-10: Learning syntax
â€¢ Iterations 11-30: Avoiding common errors
â€¢ Iterations 31-50: Handling edge cases
â€¢ Iterations 51-70: Optimizing approaches
â€¢ Iterations 71-100: Refinement
```

### Guess the Number

```
Avg Guesses
  ^
10  |â”€â”€â”€â”€â•²
    |     â•²
7   |      â•²___
    |          â•²___
4   |              â•²____
    |                   â•²â•²___
1   |                       â•²___
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Iteration
        1    10   20   30   40
        
What's happening:
â€¢ Iterations 1-10: Random guessing
â€¢ Iterations 11-20: Noticing patterns
â€¢ Iterations 21-30: Developing strategy
â€¢ Iterations 31-40: Approaching optimal (binary search)
```

---

## ğŸ² Reward Structures Visualized

### Binary Rewards (Simple)

```
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  ANSWER EVALUATION                  â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     
              â”Œâ”€â”€â”€â”€ Correct â”€â”€â†’ +1.0 âœ…
              â”‚
     Answer â”€â”€â”¤
              â”‚
              â””â”€â”€â”€â”€ Wrong â”€â”€â”€â†’  0.0 âŒ

Used in: Basic math problems, simple games
```

### Graduated Rewards (Medium)

```
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  TEST EXECUTION                     â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     
              â”Œâ”€ All tests pass â”€â”€â”€â”€â†’ +1.0 âœ…
              â”‚
              â”œâ”€ 3/4 tests pass â”€â”€â”€â†’ +0.75 âš ï¸
              â”‚
     Code  â”€â”€â”€â”¼â”€ 2/4 tests pass â”€â”€â”€â†’ +0.5 âš ï¸
              â”‚
              â”œâ”€ 1/4 tests pass â”€â”€â”€â†’ +0.25 âš ï¸
              â”‚
              â”œâ”€ 0/4 tests pass â”€â”€â”€â†’  0.0 âŒ
              â”‚
              â””â”€ Syntax error â”€â”€â”€â”€â”€â†’ -0.5 ğŸ’¥

Used in: Code generation
```

### Multi-Step Rewards (Complex)

```
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  TWENTY QUESTIONS GAME              â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     
              â”Œâ”€ Found in 1-5 Q's â”€â”€â†’ +10.0 ğŸ¯
              â”‚
              â”œâ”€ Found in 6-10 Q's â”€â†’  +7.0 ğŸ˜Š
              â”‚
     Game â”€â”€â”€â”€â”¼â”€ Found in 11-15 Q's â”€â†’  +4.0 ğŸ˜
              â”‚
              â”œâ”€ Found in 16-20 Q's â”€â†’  +1.0 ğŸ˜•
              â”‚
              â””â”€ Didn't find â”€â”€â”€â”€â”€â”€â”€â†’   0.0 â˜¹ï¸

Used in: Multi-turn games, conversations
```

---

## ğŸ¤– Single Agent vs Multi-Agent

### Single Agent (Learning Alone)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ENVIRONMENT                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚  â”‚  Fixed â”‚ Provides problems              â”‚
â”‚  â”‚ Datasetâ”‚ Returns rewards                â”‚
â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                â”‚
â”‚      â”‚                                     â”‚
â”‚      â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚  â”‚ Model  â”‚ Only thing learning            â”‚
â”‚  â”‚(Agent) â”‚                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Pros: Simple, stable
Cons: Limited to dataset diversity

Used in: Math RL, Code RL, RLVR
```

### Multi-Agent (Learning Together)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ENVIRONMENT                               â”‚
â”‚                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚ Agent 1 â”‚ â†â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚ (Learn) â”‚        â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â”‚                      â”‚
â”‚       â”‚             â”‚                      â”‚
â”‚       â†“             â”‚                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Interact                    â”‚
â”‚  â”‚ Agent 2 â”‚        â”‚                      â”‚
â”‚  â”‚  (Fix)  â”‚        â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â”‚                      â”‚
â”‚       â”‚             â”‚                      â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Pros: Agents adapt to each other
Cons: Complex, can be unstable

Used in: Twenty Questions
```

### Self-Play (Competing Agents)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GAME ENVIRONMENT                          â”‚
â”‚                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚ Agent X â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ (Learn) â”‚           â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜           â”‚                   â”‚
â”‚       â”‚            Battle!                 â”‚
â”‚       â†“                â”‚                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚                   â”‚
â”‚  â”‚ Agent O â”‚           â”‚                   â”‚
â”‚  â”‚ (Learn) â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                                            â”‚
â”‚  Winner gets +1.0                          â”‚
â”‚  Loser gets -1.0                           â”‚
â”‚  Draw gets 0.0                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Pros: Creates challenging opponents
      Discovers emergent strategies
Cons: Very complex, needs careful tuning

Used in: Tic-Tac-Toe, competitive games
```

---

## ğŸ¯ Advantage Computation Explained

### The Key Insight

```
"Don't just reward good solutions.
 Reward solutions that are BETTER THAN AVERAGE."
```

### Visual Example

```
Problem: "What is 5 + 3?"

4 Attempts:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Attempt 1: "8" â†’ Reward: 1.0                    â”‚ â† GOOD!
â”‚ Attempt 2: "7" â†’ Reward: 0.0                    â”‚
â”‚ Attempt 3: "8" â†’ Reward: 1.0                    â”‚ â† GOOD!
â”‚ Attempt 4: "9" â†’ Reward: 0.0                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Average reward = (1.0 + 0.0 + 1.0 + 0.0) / 4 = 0.5

Advantages = reward - average:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Attempt 1: 1.0 - 0.5 = +0.5   â¬† REINFORCE      â”‚
â”‚ Attempt 2: 0.0 - 0.5 = -0.5   â¬‡ DISCOURAGE     â”‚
â”‚ Attempt 3: 1.0 - 0.5 = +0.5   â¬† REINFORCE      â”‚
â”‚ Attempt 4: 0.0 - 0.5 = -0.5   â¬‡ DISCOURAGE     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Model learns:
â€¢ Do MORE of what leads to answer "8"
â€¢ Do LESS of what leads to answers "7" or "9"
```

### Why This is Better Than Raw Rewards

**Using Raw Rewards (BAD):**
```
All attempts get positive reward
â†’ Model reinforces everything
â†’ No clear signal about what's better
â†’ Slow learning
```

**Using Advantages (GOOD):**
```
Above-average gets positive signal
Below-average gets negative signal
â†’ Model knows what to improve
â†’ Clear comparison
â†’ Fast learning
```

---

## ğŸ“Š Training Progress: What You'll See

### Healthy Training

```
Metrics Dashboard:

Pass Rate:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘   70%  â†— Increasing
Mean Reward:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘   0.7  â†— Increasing  
Entropy:      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0.4  â†˜ Decreasing (good!)
KL Div:       â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0.02 â†” Stable (good!)

âœ… Good signs:
   â€¢ Pass rate going up
   â€¢ Rewards increasing
   â€¢ Entropy decreasing (model more confident)
   â€¢ KL divergence small (policy not changing too fast)
```

### Unhealthy Training

```
Metrics Dashboard:

Pass Rate:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   25%  â†” Stuck
Mean Reward:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0.25 â†” Not improving
Entropy:      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘   0.9  â†” Too high
KL Div:       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   0.5  â†— Too large!

âš ï¸ Problems:
   â€¢ Pass rate not improving â†’ Temperature too low?
   â€¢ Rewards flat â†’ Learning rate too low?
   â€¢ Entropy too high â†’ Model too uncertain
   â€¢ KL divergence large â†’ Learning rate too high!
```

---

## ğŸ¯ Hyperparameter Impact

### Learning Rate

```
Too Low (1e-6):
Progress: â•â•â•â•¸â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Slow! ğŸŒ
Time to converge: 200+ iterations

Just Right (5e-5):
Progress: â•â•â•â•â•â•â•â•â•â•â•¸â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Good! ğŸ˜Š
Time to converge: 50-100 iterations

Too High (1e-3):
Progress: â•â•â•¸â•±â•²â•±â•²â•±â•²â•±â•²â•±â•²â•±â•²â”€â”€â”€â”€â”€â”€  Unstable! ğŸŒªï¸
May never converge
```

### Temperature

```
Too Low (0.1):
Diversity: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  No exploration ğŸ˜
Samples: "Answer 1", "Answer 1", "Answer 1", "Answer 1"

Just Right (0.8):
Diversity: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘  Good variety! ğŸ˜Š
Samples: "Answer 1", "Answer 2", "Answer 1", "Answer 3"

Too High (1.5):
Diversity: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  Too random ğŸ˜µ
Samples: "qwerty", "!!!!", "...", "xyz123"
```

### Group Size

```
Too Small (1-2):
Comparisons: None! âŒ
Can't compute good advantages

Just Right (4-8):
Comparisons: Good! âœ…
Clear winners and losers

Too Large (16-32):
Comparisons: Excellent! âœ…
But slower (more compute)
```

---

## ğŸ“ Final Visual: The Learning Journey

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            FROM RANDOM TO EXPERT                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Day 1: Complete Beginner
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "What is 2+2?"                          â”‚
â”‚ Model: "Blue!" âŒ                        â”‚
â”‚ Model: "Tuesday!" âŒ                     â”‚
â”‚ Model: "4" âœ… (luck!)                    â”‚
â”‚ Model: "Cat!" âŒ                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Success rate: 25% (random)

Day 10: Learning Basics
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "What is 2+2?"                          â”‚
â”‚ Model: "4" âœ…                            â”‚
â”‚ Model: "5" âŒ (close!)                   â”‚
â”‚ Model: "4" âœ…                            â”‚
â”‚ Model: "3" âŒ (close!)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Success rate: 50% (improving)

Day 20: Competent
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "What is 2+2?"                          â”‚
â”‚ Model: "2 + 2 = 4" âœ…                    â”‚
â”‚ Model: "The sum is 4" âœ…                 â”‚
â”‚ Model: "4" âœ…                            â”‚
â”‚ Model: "Four" âœ…                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Success rate: 100% (mastered!)

Day 30: Expert (Generalizes!)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "What is 157 + 283?"                    â”‚
â”‚ Model: "157 + 283 = 440" âœ…              â”‚
â”‚ Model: "Let me calculate: 400 + 40" âœ…   â”‚
â”‚ Model: "The answer is 440" âœ…            â”‚
â”‚ Model: "440 is the sum" âœ…               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Success rate: 100% (on NEW problems!)
```

---

## ğŸ¯ Remember

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  The essence of RL in 4 words:                 â”‚
â”‚                                                â”‚
â”‚        TRY â†’ EVALUATE â†’ LEARN â†’ REPEAT         â”‚
â”‚                                                â”‚
â”‚  Everything else is just details!              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

