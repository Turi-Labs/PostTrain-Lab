{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "278d3cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tinker\n",
    "from tinker import types\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tinker_cookbook import renderers\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09a72b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"google-research-datasets/mbpp\",\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "573a2350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "- deepseek-ai/DeepSeek-V3.1\n",
      "- deepseek-ai/DeepSeek-V3.1-Base\n",
      "- moonshotai/Kimi-K2-Thinking\n",
      "- meta-llama/Llama-3.1-70B\n",
      "- meta-llama/Llama-3.1-8B\n",
      "- meta-llama/Llama-3.1-8B-Instruct\n",
      "- meta-llama/Llama-3.2-1B\n",
      "- meta-llama/Llama-3.2-3B\n",
      "- meta-llama/Llama-3.3-70B-Instruct\n",
      "- Qwen/Qwen3-235B-A22B-Instruct-2507\n",
      "- Qwen/Qwen3-30B-A3B\n",
      "- Qwen/Qwen3-30B-A3B-Base\n",
      "- Qwen/Qwen3-30B-A3B-Instruct-2507\n",
      "- Qwen/Qwen3-32B\n",
      "- Qwen/Qwen3-4B-Instruct-2507\n",
      "- Qwen/Qwen3-8B\n",
      "- Qwen/Qwen3-8B-Base\n",
      "- Qwen/Qwen3-VL-235B-A22B-Instruct\n",
      "- Qwen/Qwen3-VL-30B-A3B-Instruct\n",
      "- openai/gpt-oss-120b\n",
      "- openai/gpt-oss-20b\n"
     ]
    }
   ],
   "source": [
    "service_client = tinker.ServiceClient()\n",
    "print(\"Available models:\")\n",
    "for item in service_client.get_server_capabilities().supported_models:\n",
    "    print(\"- \" + item.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd1c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_client = service_client.create_lora_training_client(\n",
    "    base_model=\"meta-llama/Llama-3.2-1B\",\n",
    "    rank=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a29a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_sampler = service_client.create_sampling_client(\n",
    "    base_model=\"meta-llama/Llama-3.2-1B\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb576698",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = training_client.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d3af415",
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer = renderers.get_renderer(\"llama3\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c46ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_messages(example):\n",
    "    tests = \"\\n\".join(example[\"test_list\"])\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a senior Python engineer. \"\n",
    "                \"Write correct, test-passing code.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "                Problem:\n",
    "                {example['text']}\n",
    "\n",
    "                Tests:\n",
    "                {tests}\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"\"\"\n",
    "                ```python\n",
    "                {example['code']}\n",
    "                ```\n",
    "            \"\"\"\n",
    "        }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a504fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    messages = build_messages(example)\n",
    "\n",
    "    model_input, weights = renderer.build_supervised_example(messages)\n",
    "    \n",
    "    tokens = model_input.tolist()\n",
    "    weights = weights.tolist()\n",
    "\n",
    "    return types.Datum(\n",
    "        model_input=types.ModelInput.from_ints(tokens[:-1]),\n",
    "        loss_fn_inputs={\n",
    "            \"target_tokens\": tokens[1:],\n",
    "            \"weights\": weights[1:]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44ea9d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    process_example(ex)\n",
    "    for ex in dataset.select(range(200))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2bd99b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | Loss 1.5811\n",
      "Step 1 | Loss 1.5173\n",
      "Step 2 | Loss 1.3705\n",
      "Step 3 | Loss 1.2140\n",
      "Step 4 | Loss 1.0810\n",
      "Step 5 | Loss 0.9881\n",
      "Step 6 | Loss 0.9261\n",
      "Step 7 | Loss 0.8793\n",
      "Step 8 | Loss 0.8450\n",
      "Step 9 | Loss 0.8150\n",
      "Step 10 | Loss 0.7824\n",
      "Step 11 | Loss 0.7545\n",
      "Step 12 | Loss 0.7345\n",
      "Step 13 | Loss 0.7151\n",
      "Step 14 | Loss 0.6941\n",
      "Step 15 | Loss 0.6735\n",
      "Step 16 | Loss 0.6548\n",
      "Step 17 | Loss 0.6383\n",
      "Step 18 | Loss 0.6223\n",
      "Step 19 | Loss 0.6039\n",
      "Step 20 | Loss 0.5847\n",
      "Step 21 | Loss 0.5656\n",
      "Step 22 | Loss 0.5473\n",
      "Step 23 | Loss 0.5294\n",
      "Step 24 | Loss 0.5108\n",
      "Step 25 | Loss 0.4916\n",
      "Step 26 | Loss 0.4707\n",
      "Step 27 | Loss 0.4496\n",
      "Step 28 | Loss 0.4292\n",
      "Step 29 | Loss 0.4089\n",
      "Step 30 | Loss 0.3885\n",
      "Step 31 | Loss 0.3673\n",
      "Step 32 | Loss 0.3455\n",
      "Step 33 | Loss 0.3230\n",
      "Step 34 | Loss 0.3001\n",
      "Step 35 | Loss 0.2789\n",
      "Step 36 | Loss 0.2560\n",
      "Step 37 | Loss 0.2341\n",
      "Step 38 | Loss 0.2134\n",
      "Step 39 | Loss 0.1927\n",
      "Step 40 | Loss 0.1729\n",
      "Step 41 | Loss 0.1535\n",
      "Step 42 | Loss 0.1354\n",
      "Step 43 | Loss 0.1182\n",
      "Step 44 | Loss 0.1023\n",
      "Step 45 | Loss 0.0880\n",
      "Step 46 | Loss 0.0744\n",
      "Step 47 | Loss 0.0623\n",
      "Step 48 | Loss 0.0520\n",
      "Step 49 | Loss 0.0428\n"
     ]
    }
   ],
   "source": [
    "from tinker.types import AdamParams\n",
    "import numpy as np\n",
    "\n",
    "for step in range(50):\n",
    "    fwd = training_client.forward_backward(\n",
    "        training_data,\n",
    "        loss_fn=\"cross_entropy\"\n",
    "    )\n",
    "    opt = training_client.optim_step(\n",
    "        AdamParams(learning_rate=1e-4)\n",
    "    )\n",
    "\n",
    "    fwd_res = fwd.result()\n",
    "    opt.result()\n",
    "\n",
    "    logprobs = np.concatenate(\n",
    "        [o[\"logprobs\"].tolist() for o in fwd_res.loss_fn_outputs]\n",
    "    )\n",
    "    weights = np.concatenate(\n",
    "        [d.loss_fn_inputs[\"weights\"].tolist() for d in training_data]\n",
    "    )\n",
    "\n",
    "    loss = -np.dot(logprobs, weights) / weights.sum()\n",
    "    print(f\"Step {step} | Loss {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2816b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = training_client.save_weights_and_get_sampling_client(\"mbpp-sft\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfe7bc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            ```python\n",
      "            def Sum_of_Inverse_Divisors(N, M):\n",
      "    sum = 0\n",
      "    for i in range(1,N + 1):\n",
      "        j = i \n",
      "        inverse = 0\n",
      "        while (j % M == 0):\n",
      "            j = int(j / M)\n",
      "            inverse = inverse + 1\n",
      "        sum = sum + (inverse) \n",
      "    return sum\n",
      "            ```\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "example = dataset[250]\n",
    "\n",
    "prompt = renderer.build_generation_prompt([\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a senior Python engineer.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\n",
    "            Problem:\n",
    "            {example['text']}\n",
    "\n",
    "            Tests:\n",
    "            {chr(10).join(example['test_list'])}\n",
    "        \"\"\"\n",
    "    }\n",
    "])\n",
    "\n",
    "stop_sequences = renderer.get_stop_sequences()\n",
    "\n",
    "result = sampler.sample(\n",
    "    prompt=prompt,\n",
    "    num_samples=1,\n",
    "    sampling_params=types.SamplingParams(\n",
    "        max_tokens=400,\n",
    "        temperature=0.2,\n",
    "        stop=stop_sequences\n",
    "    )\n",
    ").result()\n",
    "\n",
    "tokens = result.sequences[0].tokens\n",
    "response, _ = renderer.parse_response(tokens)\n",
    "\n",
    "print(response[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5259a2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROBLEM:\n",
      "Write a python function to find sum of inverse of divisors.\n",
      "\n",
      "TESTS:\n",
      "  assert Sum_of_Inverse_Divisors(6,12) == 2\n",
      "  assert Sum_of_Inverse_Divisors(9,13) == 1.44\n",
      "  assert Sum_of_Inverse_Divisors(1,4) == 4\n",
      "================================================================================\n",
      "\n",
      "ðŸ”µ BASELINE MODEL OUTPUT:\n",
      "--------------------------------------------------------------------------------\n",
      "            Solution:\n",
      "            def Sum_of_Inverse_Divisors(divisor, divisor_count):\n",
      "                if divisor_count == 0:\n",
      "                    return 0\n",
      "                else:\n",
      "                    return (Sum_of_Inverse_Divisors(divisor, divisor_count - 1) + divisor_count)\n",
      "            print(Sum_of_Inverse_Divisors(6,12))\n",
      "            print(Sum_of_Inverse_Divisors(9,13))\n",
      "            print(Sum_of_Inverse_Divisors(1,4))\n",
      "            print(Sum_of_Inverse_Divisors(2,3))\n",
      "            print(Sum_of_Inverse_Divisors(3,2))\n",
      "            print(Sum_of_Inverse_Divisors(4,1))\n",
      "            print(Sum_of_Inverse_Divisors(5,1))\n",
      "            print(Sum_of_Inverse_Divisors(6,1))\n",
      "            print(Sum_of_Inverse_Divisors(7,1))\n",
      "            print(Sum_of_Inverse_Divisors(8,1))\n",
      "            print(Sum_of_Inverse_Divisors(9,1))\n",
      "            print(Sum_of_Inverse_Divisors(10,1))\n",
      "            print(Sum_of_Inverse_Divisors(11,1))\n",
      "            print(Sum_of_Inverse_Divisors(12,1))\n",
      "            print(Sum_of_Inverse_Divisors(13,1))\n",
      "            print(Sum_of_Inverse_Divisors(14,1))\n",
      "            print(Sum_of_Inverse_Divisors(15,1))\n",
      "            print(Sum_of_Inverse_Divisors(16,1))\n",
      "            print(Sum_of_Inverse_Divisors(17,1))\n",
      "            print(Sum_of_Inverse_Divisors(18,1))\n",
      "            print(Sum_of_Inverse_Divisors(19,1))\n",
      "            print(Sum_of_Inverse_Divisors(20,1))\n",
      "            print(Sum_of_Inverse_Divisors(21,\n",
      "\n",
      "ðŸŸ¢ TRAINED MODEL OUTPUT:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "            ```python\n",
      "            def Sum_of_Inverse_Divisors(a, b): \n",
      "    if (b == 0): \n",
      "        return -1\n",
      "    sum = -oo\n",
      "    for i in range(1, b + 1): \n",
      "        j = a * i \n",
      "        sum = max(sum, (j / i)) \n",
      "    return sum \n",
      "            ```\n",
      "        \n",
      "\n",
      "âœ… EXPECTED SOLUTION:\n",
      "--------------------------------------------------------------------------------\n",
      "```python\n",
      "def Sum_of_Inverse_Divisors(N,Sum): \n",
      "    ans = float(Sum)*1.0 /float(N);  \n",
      "    return round(ans,2); \n",
      "```\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Comparison between trained model and baseline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "test_example = dataset[250]\n",
    "\n",
    "# Build the prompt\n",
    "prompt_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a senior Python engineer. Write correct, test-passing code.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\n",
    "            Problem:\n",
    "            {test_example['text']}\n",
    "\n",
    "            Tests:\n",
    "            {chr(10).join(test_example['test_list'])}\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = renderer.build_generation_prompt(prompt_messages)\n",
    "stop_sequences = renderer.get_stop_sequences()\n",
    "\n",
    "sampling_params = types.SamplingParams(\n",
    "    max_tokens=400,\n",
    "    temperature=0.2,\n",
    "    stop=stop_sequences\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PROBLEM:\")\n",
    "print(test_example['text'])\n",
    "print(\"\\nTESTS:\")\n",
    "for test in test_example['test_list']:\n",
    "    print(f\"  {test}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Sample from baseline model\n",
    "print(\"\\nðŸ”µ BASELINE MODEL OUTPUT:\")\n",
    "print(\"-\" * 80)\n",
    "baseline_result = baseline_sampler.sample(\n",
    "    prompt=prompt,\n",
    "    num_samples=1,\n",
    "    sampling_params=sampling_params\n",
    ").result()\n",
    "\n",
    "baseline_tokens = baseline_result.sequences[0].tokens\n",
    "baseline_response, _ = renderer.parse_response(baseline_tokens)\n",
    "print(baseline_response[\"content\"])\n",
    "\n",
    "# Sample from trained model\n",
    "print(\"\\nðŸŸ¢ TRAINED MODEL OUTPUT:\")\n",
    "print(\"-\" * 80)\n",
    "trained_result = sampler.sample(\n",
    "    prompt=prompt,\n",
    "    num_samples=1,\n",
    "    sampling_params=sampling_params\n",
    ").result()\n",
    "\n",
    "trained_tokens = trained_result.sequences[0].tokens\n",
    "trained_response, _ = renderer.parse_response(trained_tokens)\n",
    "print(trained_response[\"content\"])\n",
    "\n",
    "# Show expected solution\n",
    "print(\"\\nâœ… EXPECTED SOLUTION:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"```python\\n{test_example['code']}\\n```\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
